# 🚀 加密貨幣價格預測模型優化路線圖

## 📊 問題診斷：為什麼上一個模型效果變差？

**MAE 從 0.1549 → 0.3601，性能下降 133%**

### 根本原因分析

| 問題 | 舊模型症狀 | 技術根因 |
|------|---------|--------|
| **鏡像滯後** | 黃線簡直是藍線的複製，向右平移 10+ 步 | 特徵維度災難（40 個特徵 + 1000 條數據）→ 模型學到「記憶昨天價格」而非「預測未來」 |
| **極端值崩潰** | 低價區完全散開，無法追蹤下跌 | Dropout 0.6 太強 → 模型變成「瞎子」 |
| **分佈偏離** | 預測標準差遠低於實際 | 過度正則化導致模型過度保守 |

---

## 🔧 修復方案（已實施）

### 核心調整：三個關鍵參數

#### 1️⃣ **數據量：1,000 → 10,000 K線**

**為什麼**：
- 40 個特徵需要足夠的數據來訓練
- 1,000 個樣本 = 约 40 天（太短，市場週期不完整）
- 10,000 個樣本 = 约 400 小時 = 约 17 天（包含多個市場狀態）

**期望效果**：
- ✅ 模型學到真實的市場模式而非記憶
- ✅ 減少過度擬合，提升泛化能力

#### 2️⃣ **特徵數量：40 → 20**

**移除的複雜指標**：
- ❌ Williams %R (超買超賣但容易失效)
- ❌ Money Flow Index (成交量加權複雜度高)
- ❌ Commodity Channel Index (振盪器，二階特徵)

**保留的核心特徵**（20 個）：

| 類別 | 特徵 | 個數 |
|------|------|------|
| **基礎** | open, high, low, close, volume | 5 |
| **趨勢** | SMA_10, SMA_20, EMA_12, MACD, MACD_signal | 5 |
| **動量** | RSI, ROC_1, ROC_5, Price_accel | 4 |
| **波動** | BB_upper, BB_lower, Volatility | 3 |
| **成交量** | Volume_ratio, Volume_accel | 2 |
| **隨機性** | Stoch_K, Stoch_D | 2 |
| **其他** | Daily_return, Close_position | 2 |
| **總計** | - | **20** |

**期望效果**：
- ✅ 降低維度災難（40 特徵 + 1000 樣本 → 20 特徵 + 10000 樣本）
- ✅ 保留最有效的信號，移除噪音

#### 3️⃣ **Dropout：0.6 → 0.3**

**為什麼減少 Dropout**：
- 0.6 在 1000 樣本時防止過度擬合很好
- 但在 10000 樣本時變得過度保守
- 導致模型對價格波動失敏感（預測變成直線）

**新的 Dropout 策略**（分層遞減）：
```
LSTM Dropout:      0.3
FC 第1層 Dropout:  0.3
FC 第2層 Dropout:  0.25
FC 第3層 Dropout:  0.2
FC 第4層 Dropout:  0.15
FC 第5層 Dropout:  0.1
Fusion Dropout:    0.2
```

**期望效果**：
- ✅ 模型能學到細微的市場動作
- ✅ 在深層仍保持正則化防止過度擬合
- ✅ 預測標準差更接近實際

---

## 📈 預期改進

### MAE 目標

| 階段 | 方案 | SOL MAE | BTC MAE | 進度 |
|------|------|---------|---------|------|
| **基線** | 原始模型（40 特徵，1000 數據，Dropout 0.6） | 0.3601 | 0.1684 | ✅ 已完成 |
| **第 1 次迭代** | 新方案（20 特徵，10000 數據，Dropout 0.3） | **0.20-0.25** | **0.12-0.15** | ⏳ 進行中 |
| **第 2 次迭代** | 微調超參數 + 特徵選擇 | **0.15-0.18** | **0.10-0.12** | ⏳ 待做 |
| **第 3 次迭代** | Ensemble 融合 + 多幀頻 | **< 0.15** | **< 0.10** | ⏳ 待做 |

### 具體指標改進

#### 滯後效應
- **舊**：10+ 步延遲（預測完全跟不上實際）
- **新**：3-5 步延遲（可接受）
- **目標**：1-2 步（領先型）

#### 低估偏差
- **舊**：預測分佈相對實際明顯右偏
- **新**：預測分佈對稱
- **目標**：完全對稱

#### 散點圖緊湊度
- **舊**：藍點在低價區大幅散開
- **新**：藍點緊貼回歸線
- **目標**：R² > 0.85

---

## 🏃 立即開始訓練

### SOL（推薦首先訓練）

```bash
python train_model_ultimate.py --symbol SOL --epochs 100 --device cuda
```

**預期訓練時間**：
- 10000 K線 = 更多計算量
- GPU 上大約 20-30 分鐘（vs 原來 5 分鐘）
- 但數據質量提升，最終效果值得等待

### 其他幣種

```bash
# BTC
python train_model_ultimate.py --symbol BTC --epochs 100 --device cuda

# ETH
python train_model_ultimate.py --symbol ETH --epochs 100 --device cuda

# AVAX
python train_model_ultimate.py --symbol AVAX --epochs 100 --device cuda
```

---

## 📝 代碼改動清單

✅ **已完成**：

- [x] `src/data_fetcher.py`：特徵簡化到 20 個
- [x] `train_model_ultimate.py`：更新為抓取 10000 K線
- [x] `src/model_trainer_ultimate.py`：Dropout 改為 0.3

⏳ **待做**（可選優化）：

- [ ] 超參數微調（learning_rate, batch_size）
- [ ] 特徵重要性分析（移除無用特徵）
- [ ] 多幀頻融合（1h + 4h + 1d）
- [ ] 交叉驗證（K-fold）

---

## 🔍 訓練時監控的指標

### 關鍵看板

```
Epoch  20/100 | Train: 0.045234 | Val: 0.051234 | Ratio: 1.132 ✓GOOD | LR: 3.5e-05
Epoch  40/100 | Train: 0.038921 | Val: 0.042156 | Ratio: 1.083 ✓GOOD | LR: 5.0e-05
Epoch  60/100 | Train: 0.035642 | Val: 0.039234 | Ratio: 1.101 ✓GOOD | LR: 4.8e-05
Epoch  80/100 | Train: 0.033421 | Val: 0.038456 | Ratio: 1.151 ✓GOOD | LR: 3.2e-05
```

**解讀**：
- **Ratio < 1.3**：✅ 好（正規化有效）
- **Ratio 1.3-1.6**：⚠️ 中等（接受，但可優化）
- **Ratio > 1.6**：🔴 差（過度擬合）

### 訓練後檢查清單

```
[
]
1. 最終 Validation Loss 是否 < 0.05？
[ ] 2. Overfitting Ratio 是否穩定在 1.1-1.3？
[ ] 3. MAE 是否 < 0.25（SOL）？
[ ] 4. 預測曲線是否跟實際在 3-5 步內？
[ ] 5. 低價區散點是否聚集？
```

---

## 🎯 下一步

### 第 1 步：訓練（現在）
```bash
python train_model_ultimate.py --symbol SOL --epochs 100 --device cuda
```

### 第 2 步：評估（訓練後）
```bash
python visualize_predictions_ultimate.py --symbol SOL
```

### 第 3 步：對比
- 新 MAE 是否 < 0.25？
- 滯後是否改善？
- 散點圖是否更緊湊？

### 第 4 步：部署
- 如果效果好，用新模型替換生產環境
- 監控實時預測信號質量

---

## 📚 關鍵洞察

### 為什麼 40 特徵 + 1000 樣本失敗？

**維度災難（Curse of Dimensionality）**：
- 特徵數 / 樣本數 = 40 / 1000 = 0.04
- 一般規則：需要 > 0.001 (100:1 樣本數:特徵數)
- 你的比例勉強達標，但實際模型需要 500:1 才穩定

**解決方案**：
- 提升樣本數（1000 → 10000）：比例變成 0.002（好 2 倍）✅
- 降低特徵數（40 → 20）：比例變成 0.002（同樣好 2 倍）✅

### Dropout 0.6 的問題

- 在訓練集充足時（>5000），Dropout 0.3 更合適
- 0.6 是針對 <1000 樣本的激進防止過度擬合
- 你現在有 10000，所以可以用更溫和的 0.3

---

**最後更新**：2025-12-13  
**狀態**：準備第 1 次迭代訓練  
**目標**：MAE 從 0.36 改善到 0.20-0.25
